---
title: "Lab 9 HPC"
author: "Michelle Hernandez"
format:
  html:
    embed-resources: true
editor: visual
---

```{r}
library(parallel)
library(microbenchmark)
```

## **Problem 1: Vectorization**

The following functions can be written to be more efficient without using parallel. Write a faster version of each function and show that (1) the outputs are the same as the slow version, and (2) your version is faster.

1.  This function generates an `n x k` dataset with all its entries drawn from a Poission distribution with mean `lambda`.

    the given code is saying to draw 4 samples from a pois dist with mean 4 then do this 100 times and bind it to x so you get 100 rows with 4 columns , generating the 4 numbers 100 times

```{r}

fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n){
    x <- rbind(x, rpois(k, lambda))    
  }
  
  return(x)
}
result <- fun1()
#print(result)



fun1alt <- function(n = 100, k = 4, lambda = 4) {
  x <- matrix(rpois(n * k, lambda), nrow = n, ncol = k)
  return(x)
}
resultalt <-fun1alt()
#print(resultalt)
```

Show that `fun1alt` generates a matrix with the same dimensions as `fun1` and that the values inside the two matrices follow similar distributions. Then check the speed of the two functions with the following code:

```{r}
microbenchmark::microbenchmark(
  fun1(),
  fun1alt()
)

dim(result)
dim(resultalt)
mean(result[1:100,])
mean(resultalt[1:100,])

hist(result)
hist(resultalt)
```

Comments:

2.  This function finds the maximum value of each column of a matrix (hint: check out the `max.col()` function).

```{r}
library(matrixStats)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}
result2 <- fun2(x)

fun2alt <- function(x) {
  max_val <- colMaxs(x)
}
result2alt <- fun2alt(x)

```

```{r}
microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x)
)


dim(result2)
dim(result2alt)


hist(result2)
hist(result2alt)
```

## **Problem 3: Parallelization**

We will now turn our attention to the statistical concept of [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29). Among its many uses, non-parametric bootstrapping allows us to obtain confidence intervals for parameter estimates without relying on parametric assumptions. Don't worry if these concepts are unfamiliar, we only care about the computation methods in this lab, not the statistics.

The main assumption is that we can approximate the results of many repeated experiments by resampling observations from our original dataset, which reflects the population.

1.  This function implements a serial version of the bootstrap. Edit this function to parallelize the `lapply` loop, using whichever method you prefer. Rather than specifying the number of cores to use, use the number given by the `ncpus` argument, so that we can test it with different numbers of cores later.

```{r}

my_boot <- function(dat, stat, R, ncpus = 1L) {
  

  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)

  ans <- mclapply(1:1L,
                  function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  }, mc.cores=1L)
    
  ans <- do.call(rbind, ans)

  return(ans)
}


```

2.Once you have a version of the `my_boot()` function that runs on multiple cores, check that it provides accurate results by comparing it to a parametric model:

```{r}
# Bootstrap of an OLS
my_stat <- function(d) coef(lm(y ~ x, data=d))

# DATA SIM
set.seed(1)
n <- 500; R <- 1e4

x <- cbind(rnorm(n)); y <- x*5 + rnorm(n)

# Checking if we get something similar as lm
ans0 <- confint(lm(y~x))
ans1 <- my_boot(dat = data.frame(x, y), stat = my_stat, R = R, ncpus = 2L)

t(apply(ans1, 2, quantile, c(.025,.975)))
ans0


# You should get something like this for t(apply

##                   2.5%      97.5%
## (Intercept) -0.1372435 0.05074397
## x            4.8680977 5.04539763

#look like this for ans0
##                  2.5 %     97.5 %
## (Intercept) -0.1379033 0.04797344
## x            4.8650100 5.04883353
```

3.  Check whether your version actually goes faster when it's run on multiple cores (since this might take a little while to run, we'll use `system.time` and just run each version once, rather than `microbenchmark`, which would run each version 100 times, by default):

```{r}
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 2L))

```
